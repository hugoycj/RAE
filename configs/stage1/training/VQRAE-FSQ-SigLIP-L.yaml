stage_1:
  target: stage1.VQRAE
  params:
    # Encoder configuration - SigLIP-L (400M parameters)
    encoder_cls: 'SigLIP2wNorm'
    encoder_config_path: 'google/siglip-large-patch16-384'  # SigLIP-L
    encoder_input_size: 224  # Training resolution
    encoder_params: 
      model_name: 'google/siglip-large-patch16-384'
      num_tokens: 196  # 14x14 patches for 224x224 input
    
    # Decoder configuration - Symmetric ViT (mirrors encoder)
    decoder_config_path: 'configs/decoder/ViTXL'  # Same depth as SigLIP-L (24 layers)
    decoder_patch_size: 16
    
    # FSQ (Finite Scalar Quantization) parameters
    use_fsq: true  # Use FSQ instead of VQ/SimVQ
    fsq_levels: [8, 8, 8]  # 8^3 = 512 implicit codebook entries (3D FSQ)
    fsq_use_projection: true  # Use projection layer to map 768D latent -> 3D FSQ -> 768D
    
    # Alternative FSQ configurations for 16384 codebook size:
    # fsq_levels: [8, 8, 16, 16]  # 8*8*16*16 = 16384 codes (4D FSQ)
    # fsq_levels: [8, 8, 8, 8, 4]  # 8*8*8*8*4 = 16384 codes (5D FSQ)
    
    # Other common FSQ configurations:
    # fsq_levels: [8, 6, 5]  # 8*6*5 = 240 codes (3D FSQ)
    # fsq_levels: [7, 5, 5, 5]  # 7*5*5*5 = 875 codes (4D FSQ)
    # fsq_levels: [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes (4D FSQ)
    
    # Note: With fsq_use_projection=true, the projection layer automatically
    # maps between the encoder's latent dimension (e.g., 768 for SigLIP-L)
    # and the FSQ dimension (e.g., 3 for [8,8,8]).
    # Set fsq_use_projection=false only if fsq_levels length equals latent_dim.
    
    vq_epsilon: 1e-3  # Small epsilon for FSQ numerical stability
    quantize_before_reshape: false  # Quantize in 2D format
    freeze_encoder: true  # Stage-1: Freeze encoder
    
    # RAE parameters
    noise_tau: 0.0  # No noise injection for VQRAE
    reshape_to_2d: true
    normalization_stat_path: null  # Computed during training

# Training configuration
training:
  epochs: 16
  ema_decay: 0.9978
  batch_size: 32 # per-proc bs
  num_workers: 8
  clip_grad: 0.0
  log_interval: 100
  checkpoint_interval: 5000
  sample_interval: 0
  optimizer:
    lr: 2.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.0
  scheduler:
    type: cosine
    warmup_epochs: 1
    decay_end_epoch: 16
    base_lr: 2.0e-4
    final_lr: 2.0e-5

gan:
  disc:
    arch:
      dino_ckpt_path: 'models/discs/dino_vit_small_patch8_224.pth'
      ks: 9
      norm_type: 'bn'
      using_spec_norm: true
      recipe: 'S_8'
    optimizer:
      lr: 2.0e-4
      betas: [0.9, 0.95]
      weight_decay: 0.0
    scheduler:
      type: cosine
      warmup_epochs: 1
      decay_end_epoch: 16
      base_lr: 2.0e-4
      final_lr: 2.0e-5
    augment:
      prob: 1.0
      cutout: 0.0
  loss:
    disc_loss: hinge
    gen_loss: vanilla
    disc_weight: 0.75
    perceptual_weight: 1.0
    disc_start: 8
    disc_upd_start: 6
    lpips_start: 0
    max_d_weight: 10000.0
    disc_updates: 1
