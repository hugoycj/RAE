stage_1:
  target: stage1.VQRAE
  params:
    # Encoder configuration - SigLIP-L (unfrozen in Stage-2)
    encoder_cls: 'SigLIP2wNorm'
    encoder_config_path: 'google/siglip-large-patch16-384'
    encoder_input_size: 224
    encoder_params: 
      model_name: 'google/siglip-large-patch16-384'
      num_tokens: 196
    
    # Decoder configuration
    decoder_config_path: 'configs/decoder/ViTL_symmetric'
    decoder_patch_size: 16
    
    # Vector Quantization parameters
    num_embeddings: 16384
    commitment_cost: 0.25
    vq_decay: 0.99
    vq_epsilon: 1e-5
    quantize_before_reshape: false
    use_simvq: true
    
    # RAE parameters
    noise_tau: 0.0
    reshape_to_2d: true
    normalization_stat_path: null
    
    # Stage-2 specific: unfreeze encoder
    freeze_encoder: false  # KEY: Encoder is unfrozen in Stage-2

# Training configuration for Stage-2 (Unfrozen Encoder + Distillation)
training:
  stage: 2
  global_seed: 42
  
  # Continue from Stage-1 checkpoint
  resume_from_stage1: true
  stage1_ckpt: 'models/vqrae/stage1/checkpoint_final.pt'
  
  # Dataset
  dataset: 'datacomp1b'
  num_epochs: 10  # Fewer epochs for fine-tuning
  
  # Batch configuration
  batch_size: 512  # Per-GPU
  num_workers: 8
  gradient_accumulation_steps: 1
  
  # Optimizer settings (Stage-2) - Much lower LR!
  learning_rate: 1.0e-5  # 1e-5 for full model fine-tuning
  optimizer: 'AdamW'
  weight_decay: 0.05
  betas: [0.9, 0.95]
  
  # Learning rate schedule
  lr_warmup_steps: 1000
  lr_schedule: 'cosine'
  min_lr_ratio: 0.1
  
  # Loss weights (Stage-2) - Focus on distillation
  recon_weight: 1.0  # L2 reconstruction
  lpips_weight: 0.5  # Reduced perceptual loss
  gan_weight: 0.0  # No GAN loss in Stage-2
  vq_weight: 1.0  # VQ loss
  distill_weight: 2.0  # NEW: Self-distillation loss (key for Stage-2)
  
  # Distillation configuration
  distillation:
    enabled: true
    method: 'feature_distillation'
    # L2 loss on continuous features â†’ quantized features
    # Preserves semantic understanding from frozen encoder
    continuous_to_quantized: true
    distill_loss: 'l2'
    temperature: 1.0  # No temperature scaling for L2
  
  # EMA settings
  use_ema: true
  ema_decay: 0.9999
  
  # Checkpointing
  ckpt_every: 2500
  log_every: 100
  sample_every: 1000
  
  # Precision
  precision: 'bf16'

# No GAN in Stage-2 (disabled by gan_weight: 0.0)
# LPIPS is reduced but still used
lpips:
  network: 'vgg'
  lpips_start: 0
